{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gidGUmq7Cyff"
   },
   "source": [
    "В этой домашке задания независимы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mljAB7pCyfi"
   },
   "source": [
    "# Dropout (2 балла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rzC_iHuACyfj"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXf_CLElCyfk"
   },
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "num_classes = 10\n",
    "batch_size = 128\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./MNIST/', \n",
    "                                   train=True, \n",
    "                                   transform=transforms.ToTensor(),\n",
    "                                   download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./MNIST/', \n",
    "                                  train=False, \n",
    "                                  transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R8FGcrGyCyfl"
   },
   "outputs": [],
   "source": [
    "def plot_history(train_history, val_history, title='loss'):\n",
    "    plt.figure()\n",
    "    plt.title('{}'.format(title))\n",
    "    plt.plot(train_history, label='train', zorder=1)\n",
    "    \n",
    "    points = np.array(val_history)\n",
    "    \n",
    "    plt.scatter(points[:, 0], points[:, 1], marker='+', s=180, c='orange', label='val', zorder=2)\n",
    "    plt.xlabel('train steps')\n",
    "    \n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def train(model, opt, n_epochs):\n",
    "    train_log, train_acc_log = [], []\n",
    "    val_log, val_acc_log = [], []\n",
    "\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss, train_acc = train_epoch(model, opt, batchsize=batch_size)\n",
    "\n",
    "        val_loss, val_acc = test(model)\n",
    "\n",
    "        train_log.extend(train_loss)\n",
    "        train_acc_log.extend(train_acc)\n",
    "\n",
    "        steps = train_dataset.train_labels.shape[0] / batch_size\n",
    "        val_log.append((steps * (epoch + 1), np.mean(val_loss)))\n",
    "        val_acc_log.append((steps * (epoch + 1), np.mean(val_acc)))\n",
    "\n",
    "        clear_output()\n",
    "        plot_history(train_log, val_log)    \n",
    "        plot_history(train_acc_log, val_acc_log, title='accuracy')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UiaquD5MCyfl"
   },
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    loss_log, acc_log = [], []\n",
    "    model.eval()\n",
    "    for batch_num, (x_batch, y_batch) in enumerate(test_loader):    \n",
    "        data = Variable(x_batch)\n",
    "        target = Variable(y_batch)\n",
    "\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target).cpu()\n",
    "\n",
    "        pred = torch.max(output, 1)[1].data.numpy()\n",
    "        acc = np.mean(pred == y_batch)\n",
    "        acc_log.append(acc)\n",
    "        \n",
    "        loss = loss.item()\n",
    "        loss_log.append(loss)\n",
    "    return loss_log, acc_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MNGAwjPSCyfm"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, batchsize=32):\n",
    "    loss_log, acc_log = [], []\n",
    "    model.train()\n",
    "    for batch_num, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        data = Variable(x_batch)\n",
    "        target = Variable(y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        pred = torch.max(output, 1)[1].data.numpy()\n",
    "        acc = np.mean(pred == y_batch)\n",
    "        acc_log.append(acc)\n",
    "        \n",
    "        loss = F.nll_loss(output, target).cpu()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss = loss.item()\n",
    "        loss_log.append(loss)\n",
    "    return loss_log, acc_log   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzOCcwHGCyfm"
   },
   "source": [
    "Создайте простейшую однослойную модель - однослойную полносвязную сеть и обучите ее. Поскольку мнист это задача классификации, не забудте сделать последним слоем логсофтмакс для псевдовероятности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "675zXKV2Cyfn"
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size()[0], -1)\n",
    "    \n",
    "model = nn.Sequential(\n",
    "    Flatten(),\n",
    "    #code here\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cX79JZGjCyfn"
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "train(model, opt, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJ4GvY6_Cyfo"
   },
   "source": [
    "Параметром обученной нейросети является матрица весов, в которой каждому классу соответствует один из 784-мерных столбцов. Их можно представить как двумерные изображения 28-28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JVZEu16pCyfo"
   },
   "outputs": [],
   "source": [
    "weights = list(model.parameters())[0].data.numpy()\n",
    "plt.figure(figsize=[10, 10])\n",
    "for i in range(10):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.title(\"Label: %i\" % i)\n",
    "    plt.imshow(weights[i].reshape([28, 28]), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Hp4TJ_MCyfo"
   },
   "source": [
    "Добавьте Dropout-слой в архитектуру сети, проведите оптимизацию с параметрами, заданными ранее, визуализируйте обученные веса. Есть ли разница между весами, обученными с Dropout и без него? Параметр Dropout возьмите равным 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "coh3c1DICyfo"
   },
   "outputs": [],
   "source": [
    "class DropoutLayer(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.training:\n",
    "            #code here, здесь нужно создать тензор из биномиального распределения\n",
    "            dropout_factor = \n",
    "        else:\n",
    "            #code here, здесь тензоры не нужны\n",
    "            dropout_factor = \n",
    "        return #примените дропаут слой, смотрите torch.mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yFnI1yU-Cyfp"
   },
   "outputs": [],
   "source": [
    "p = 0.7\n",
    "\n",
    "modelDp = nn.Sequential(\n",
    "    Flatten(),\n",
    "    DropoutLayer(p),\n",
    "    #code here\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z_lK49kTCyfp"
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(modelDp.parameters(), lr=0.0005)\n",
    "train(modelDp, opt, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5TJBbE9NCyfp"
   },
   "outputs": [],
   "source": [
    "weights = list(modelDp.parameters())[0].data.numpy()\n",
    "plt.figure(figsize=[10, 10])\n",
    "for i in range(10):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.title(\"Label: %i\" % i)\n",
    "    plt.imshow(weights[i].reshape([28, 28]), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBGJcK0LCyfp"
   },
   "source": [
    "# Metric Learning (3 балла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QeTkLRnGCyfq"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import Sampler, BatchSampler\n",
    "from torch.nn.modules.loss import MSELoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYFB5ZUWCyfq"
   },
   "source": [
    "Contrastive Loss - одна из самых популярных функций потерь для metric learning. Contrastive Loss получает на вход пару векторов $x_i$ и $x_j$ и метку $y_{ij}$, причем $y_{ij} = 0$, если объекты \"похожи\" (принадлежат одному классу), и $y_{ij} = 1$, если объекты \"различны\" (принадлежат различным классам). Формально определим Contrastive Loss следующим образом:\n",
    "\n",
    "$$\n",
    "L(x_i, x_j, y_{ij}) = (1 - y_{ij})\\|x_i - x_j\\|^2 + y_{ij}max(0, m - \\|x_i - x_j\\|^2)\n",
    "$$\n",
    "\n",
    "где $m$ - гиперпараметр (его можно взять равным единице).\n",
    "\n",
    "Вместо того, чтобы формировать обучающее множество из всевозможных пар, можно поступить проще: будем пропускать батч из $N$ обучаюших изображений через сеть (тем самым получая соответствующие векторы $x$), а значение лосса вычислять как среднее значение функции $L$ на всех парах в этом батче. Тогда в обучении на каждом батче участвует $\\frac{N(N-1)}{2}$ пар, что существенно ускоряет сходимость на практике. Реализуйте предложенный вариант Contrastive Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31nmse-JCyfq"
   },
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        #code here\n",
    "        return #code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMbtSwcOCyfq"
   },
   "source": [
    "В задачах metric learning, как правило, необходимо, чтобы количества \"положительных\" и \"отрицательных\" пар в обучении отличалось несильно. Поэтому в случае большого количества классов случайное формирование батчей неэффективно - в таком случае количество \"положительных\" пар очень мало. Поэтому будем формировать обучающие батчи размера $N$ следующим образом: будем брать $\\frac{N}{2}$ элементов из некоторого класса (они между собой будут формировать \"положительные пары\"), а оставшиеся $\\frac{N}{2}$ элементов будем брать случайно. Таким образом мы гарантируем, что в каждом обучающем батче будет достаточно \"положительных\" пар."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZ-QDkFrCyfq"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LpPHReKfCyfr"
   },
   "source": [
    "В этом задании будем работать с небольшими изображениями одежды из датасета Fashion-MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oxViw1knCyfr"
   },
   "outputs": [],
   "source": [
    "class ContrastiveSampler(BatchSampler):\n",
    "    def __init__(self, batch_size, num_classes, labels):\n",
    "        self.num_classes = num_classes\n",
    "        self.imgs_per_class = labels.size()[0] // num_classes\n",
    "        class2idx = defaultdict(list)\n",
    "        for i, y in enumerate(labels):\n",
    "            class2idx[i].append(y)\n",
    "        self.batch_size = batch_size\n",
    "        self.class2idx = class2idx\n",
    "        self.total_samples = len(labels)\n",
    "\n",
    "    def __iter__(self):\n",
    "        num_yielded = 0\n",
    "        while num_yielded < (self.num_classes * self.imgs_per_class):\n",
    "            batch = []\n",
    "            base = np.random.randint(0, self.num_classes)\n",
    "            pos = np.random.choice(self.class2idx[base], size=self.batch_size // 2).tolist()\n",
    "            neg = np.random.randint(self.total_samples, size=self.batch_size // 2).tolist()\n",
    "            batch += pos + neg\n",
    "            num_yielded += self.batch_size\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sa8HC0IjCyfr"
   },
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "num_classes = 10\n",
    "batch_size = 256\n",
    "\n",
    "contrastive_loss = ContrastiveLoss()\n",
    "\n",
    "train_dataset = dsets.FashionMNIST(root='.', \n",
    "                                   train=True, \n",
    "                                   transform=transforms.ToTensor(),\n",
    "                                   download=True)\n",
    "\n",
    "test_dataset = dsets.FashionMNIST(root='.', \n",
    "                                  train=False, \n",
    "                                  transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_sampler=ContrastiveSampler(batch_size=batch_size, \n",
    "                                           num_classes=num_classes, labels=train_dataset.train_labels), \n",
    "                                           shuffle=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_sampler=ContrastiveSampler(batch_size=batch_size, \n",
    "                                          num_classes=num_classes, labels=test_dataset.test_labels), \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYLyNMqpCyfr"
   },
   "source": [
    "Реализуйте сеть несложной архитектуры, содержащую три сверточных слоя из 20 фильтров с макс-пулингом, а также два полносвязных слоя из 128 нейронов. Выход последнего слоя будет подаваться на вход Contrastive Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9kjI2VcLCyfs"
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size()[0], -1)\n",
    "\n",
    "class ContrastiveNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            #code here\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.cnn1(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n1NffcTRCyfs"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer):\n",
    "    loss_log = []\n",
    "    model.train()\n",
    "    for batch_num, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        data = Variable(x_batch)\n",
    "        target = Variable(y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)        \n",
    "        loss = contrastive_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss = loss.item()\n",
    "        loss_log.append(loss)\n",
    "    return loss_log   \n",
    "\n",
    "def test(model):\n",
    "    loss_log = []\n",
    "    model.eval()\n",
    "    for batch_num, (x_batch, y_batch) in enumerate(test_loader):    \n",
    "        data = Variable(x_batch)\n",
    "        target = Variable(y_batch)\n",
    "        output = model(data)\n",
    "        loss = contrastive_loss(output, target)\n",
    "        loss = loss.item()\n",
    "        loss_log.append(loss)\n",
    "    return loss_log\n",
    "\n",
    "def plot_history(train_history, val_history, title='loss'):\n",
    "    plt.figure()\n",
    "    plt.title('{}'.format(title))\n",
    "    plt.plot(train_history, label='train', zorder=1)    \n",
    "    points = np.array(val_history)\n",
    "    plt.scatter(points[:, 0], points[:, 1], marker='+', s=180, c='orange', label='val', zorder=2)\n",
    "    plt.xlabel('train steps')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "def train(model, opt, n_epochs):\n",
    "    train_log = []\n",
    "    val_log = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = train_epoch(model, opt)\n",
    "        val_loss = test(model)\n",
    "        train_log.extend(train_loss)\n",
    "        steps = train_dataset.train_labels.shape[0] / batch_size\n",
    "        val_log.append((steps * (epoch + 1), np.mean(val_loss)))\n",
    "        clear_output()\n",
    "        plot_history(train_log, val_log)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vD0kINapCyfs"
   },
   "outputs": [],
   "source": [
    "model = ContrastiveNetwork()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbWxbk11Cyfs"
   },
   "source": [
    "Обучите сеть с параметрами, указанными ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2QB-luiCyft"
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "train(model, opt, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FjM7gbLUCyft"
   },
   "source": [
    "Извлеките векторные описания тестовых изображений (a.k.a эмбеддинги). У вас должно получиться 10000 128-мерных векторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p3rPeEAQCyft"
   },
   "outputs": [],
   "source": [
    "data = test_dataset.test_data\n",
    "embeddings = model(Variable(data.view(-1, 1, 28, 28)).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZjx4QwYCyft"
   },
   "source": [
    "Код ниже демонстрирует поисковую выдачу для трех изображений-запросов. Выдача формируется на основе близости эмбеддингов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xeLtRQzTCyft"
   },
   "outputs": [],
   "source": [
    "queryCount = 3\n",
    "queries = embeddings[:queryCount,:].data.numpy()\n",
    "database = embeddings[queryCount:,:].data.numpy()\n",
    "plt.figure(figsize=[15, 4.5])\n",
    "for i in range(queryCount):\n",
    "    results = np.argsort(np.sum((database-queries[i,:])**2, axis=1))[:10]\n",
    "    plt.subplot(queryCount, 11, i * 11 + 1)\n",
    "    plt.title(\"Query: %i\" % i)\n",
    "    plt.imshow(test_dataset.test_data[i].numpy().reshape([28, 28]), cmap='gray')\n",
    "    for k in range(10):\n",
    "        plt.subplot(queryCount, 11, i * 11 + k + 2)\n",
    "        plt.imshow(test_dataset.test_data[results[k]+queryCount].numpy().reshape([28, 28]), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aI5-BmzCyfu"
   },
   "source": [
    "# Super-resolution (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsVXbJv1Cyfu"
   },
   "source": [
    "В этой части вам предстоит реализовать простейшую архитектуру для решения задачи image super-resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7SUBRRQCyfu"
   },
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "num_classes = 10\n",
    "batch_size = 256\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./MNIST/', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./MNIST/', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size,         \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SpKYLp8Cyfu"
   },
   "source": [
    "Мы будем увеличивать изображения размера (14,14) в два раза по каждому измерению. Как правило, перед подачей на вход нейросети изображение низкого разрешения увеличивают до нужного размера билинейной интерполяцией, а нейросеть улучшает результат интерпляции, не меняя пространственные размеры изображения.\n",
    "\n",
    "Реализуйте нейросеть из трех сверточных слоев, которая будет получать на вход черно-белое изображение и выдавать на выход изображение такого же размера. Нейросеть должна предсказывать добавку, которую необходимо прибавить к полученному на вход изображению низкого качества. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IHl8mEZYCyfu"
   },
   "outputs": [],
   "source": [
    "class SuperResolutionNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            #code here\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.cnn1(x)\n",
    "        return output + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YCC95tgrCyfu"
   },
   "outputs": [],
   "source": [
    "model = SuperResolutionNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cim9bgJ0Cyfv"
   },
   "outputs": [],
   "source": [
    "def low_res_and_high_res(images_batch):\n",
    "    result = images_batch.clone()\n",
    "    low_res_transform = transforms.Resize((14,14))\n",
    "    high_res_transform = transforms.Resize((28,28))\n",
    "    toTensorTransform = transforms.ToTensor()\n",
    "    toImageTransform = transforms.ToPILImage()\n",
    "    for i in range(images_batch.size()[0]):\n",
    "        result[i] = toTensorTransform(high_res_transform(low_res_transform(toImageTransform(images_batch[i]))))\n",
    "    return result\n",
    "\n",
    "def train_epoch(model, optimizer, batchsize=32):\n",
    "    loss_log = []\n",
    "    model.train()\n",
    "    for batch_num, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        x_batch = x_batch.float() / 255\n",
    "        data = Variable(low_res_and_high_res(x_batch))\n",
    "        target = Variable(x_batch)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)        \n",
    "        loss = F.mse_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss = loss.data.item()\n",
    "        loss_log.append(loss)\n",
    "    return loss_log   \n",
    "\n",
    "def test(model):\n",
    "    loss_log = []\n",
    "    model.eval()\n",
    "    for batch_num, (x_batch, y_batch) in enumerate(test_loader):    \n",
    "        x_batch = x_batch.float() / 255\n",
    "        data = Variable(low_res_and_high_res(x_batch))\n",
    "        target = Variable(x_batch)\n",
    "        output = model(data)\n",
    "        loss = F.mse_loss(output, target)        \n",
    "        loss = loss.data.item()\n",
    "        loss_log.append(loss)\n",
    "    return loss_log\n",
    "\n",
    "def plot_history(train_history, val_history, title='loss'):\n",
    "    plt.figure()\n",
    "    plt.title('{}'.format(title))\n",
    "    plt.plot(train_history, label='train', zorder=1)\n",
    "    points = np.array(val_history)\n",
    "    plt.scatter(points[:, 0], points[:, 1], marker='+', s=180, c='orange', label='val', zorder=2)\n",
    "    plt.xlabel('train steps')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "def train(model, opt, n_epochs):\n",
    "    train_log = []\n",
    "    val_log = []\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = train_epoch(model, opt, batchsize=batch_size)\n",
    "        val_loss = test(model)\n",
    "        train_log.extend(train_loss)\n",
    "        steps = train_dataset.train_labels.shape[0] / batch_size\n",
    "        val_log.append((steps * (epoch + 1), np.mean(val_loss)))\n",
    "        clear_output()\n",
    "        plot_history(train_log, val_log)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z30x1lzBCyfv"
   },
   "source": [
    "Оптимизируйте сеть с параметрами, указанными ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HSfUfNKcCyfv"
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=0.05)\n",
    "train(model, opt, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CSEQLJgvCyfv"
   },
   "outputs": [],
   "source": [
    "test_images = test_dataset.test_data.float() / 255\n",
    "result_cnn = model(Variable(low_res_and_high_res(test_images.view(-1,1,28,28))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sK1hM1WSCyfv"
   },
   "source": [
    "Код ниже визуализирует исходные изображения (28,28) и реконструкции, полученные с помощью нейросети.\n",
    "Не удивляйтесь, есть качество реконструкций покажется низким, MSE-loss, не является оптимальным для задачи super-resolution (гораздо лучше работают GANы, расскажу позже)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ob5Ixm9HCyfv"
   },
   "outputs": [],
   "source": [
    "examplesCount = 6\n",
    "plt.figure(figsize=[10, 10])\n",
    "for i in range(examplesCount):\n",
    "    plt.subplot(examplesCount, 2, i * 2 + 1)\n",
    "    plt.title(\"Original: %i\" % i)\n",
    "    plt.imshow(test_dataset.test_data[i].numpy().reshape([28, 28]), cmap='gray')\n",
    "    plt.subplot(examplesCount, 2, i * 2 + 2)\n",
    "    plt.title(\"Upscaled-CNN: %i\" % i)\n",
    "    plt.imshow(result_cnn[i].data.numpy().reshape([28, 28]), cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "task13_cnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
